apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceServiceConfig
metadata:
  name: llm-d-nvidia-nixl
  namespace: kserve
spec:
  template:
    initContainers:
      - name: llm-d-routing-sidecar
        image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
        restartPolicy: Always
        ports:
          - name: http
            containerPort: 8080
            protocol: TCP
        args:
        - "--port=8080"
        - "--vllm-port=8081"
    containers:
      - name: main
        image: ghcr.io/llm-d/llm-d:0.0.8
        command:
          - vllm
          - serve
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
        args:
          - "--port"
          - "8081"
          - "--enforce-eager"
        env:
          - name: CUDA_VISIBLE_DEVICES
            value: "0"
          - name: UCX_TLS
            value: "cuda_ipc,cuda_copy,tcp"
          - name: HF_HOME
            value: /model-cache
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: VLLM_NIXL_SIDE_CHANNEL_PORT
            value: "5557"
          - name: VLLM_LOGGING_LEVEL
            value: INFO
        ports:
          - name: nixl
            containerPort: 5557
            protocol: TCP
          - name: http
            containerPort: 8081
            protocol: TCP

