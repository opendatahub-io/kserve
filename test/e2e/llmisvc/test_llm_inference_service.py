# Copyright 2025 The KServe Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import os
import time
from dataclasses import dataclass
from typing import Any, Callable, List, Optional

import pytest
import requests
from kubernetes import client
from kserve import KServeClient, V1alpha1LLMInferenceService, constants

from .fixtures import (
    LLMINFERENCESERVICE_CONFIGS,
    generate_test_id,
    llm_config_factory,  # noqa: F401,F811
    test_case,  # noqa: F401,F811
    KSERVE_TEST_NAMESPACE,
)

KSERVE_PLURAL_LLMINFERENCESERVICE = "llminferenceservices"


def assert_200(response: requests.Response) -> None:
    """Default response assertion that checks for 200 status code."""
    assert (
        response.status_code == 200
    ), f"Service returned {response.status_code}: {response.text}"


@dataclass
class Case:
    """Test case configuration for LLM inference service tests."""
    base_refs: List[str]
    prompt: str = "Boston is a"
    max_tokens: int = 1
    response_assertion: Callable[[requests.Response], None] = assert_200
    llm_service: V1alpha1LLMInferenceService = None # Generated by llm_service_factory
    model_name: str = "default/model"


@pytest.mark.llminferenceservice
@pytest.mark.asyncio(loop_scope="session")
@pytest.mark.parametrize(
    "test_case",
    [
        pytest.param(
            Case(["router-managed", "workload-single-cpu", "model-fb-opt-125m"]),
            marks=pytest.mark.cluster_cpu,
        ),
        pytest.param(
            Case(
                base_refs=["router-managed", "workload-single-cpu", "model-fb-opt-125m"],
                prompt="What is the capital of France?",
                response_assertion=lambda response: (
                    response.status_code == 200 
                    and response.json().get("choices") is not None
                    and len(response.json().get("choices", [])) > 0
                ),
            ),
            marks=pytest.mark.cluster_cpu,
        ),
    ],
    indirect=["test_case"],
    ids=generate_test_id,
)
def test_llm_inference_service(test_case: Case):
    
    kserve_client = KServeClient(
        config_file=os.environ.get("KUBECONFIG", "~/.kube/config")
    )

    service_name = test_case.llm_service.metadata.name
    
    try:
        create_llmisvc(kserve_client, test_case.llm_service)
        wait_for_model_response(kserve_client, test_case)
    except Exception as e:
        print(f"ERROR: Failed to call llm inference service {service_name}: {e}")
        collect_diagnostics(kserve_client, test_case.llm_service)
        raise
    finally:
        try:
            delete_llmisvc(kserve_client, test_case.llm_service)
        except Exception as e:
            print(f"Warning: Failed to cleanup service {service_name}: {e}")


def create_llmisvc(kserve_client: KServeClient, llm_isvc: V1alpha1LLMInferenceService):
    from kserve.utils import utils

    try:
        outputs = kserve_client.api_instance.create_namespaced_custom_object(
            constants.KSERVE_GROUP,
            llm_isvc.api_version.split("/")[1],
            llm_isvc.metadata.namespace,
            KSERVE_PLURAL_LLMINFERENCESERVICE,
            llm_isvc,
        )
        return outputs
    except client.rest.ApiException as e:
        raise RuntimeError(
            f"Exception when calling CustomObjectsApi->"
            f"create_namespaced_custom_object for LLMInferenceService: {e}"
        ) from e


def delete_llmisvc(kserve_client: KServeClient, llm_isvc: V1alpha1LLMInferenceService):
    try:
        return kserve_client.api_instance.delete_namespaced_custom_object(
            constants.KSERVE_GROUP,
            llm_isvc.api_version.split("/")[1],
            llm_isvc.metadata.namespace,
            KSERVE_PLURAL_LLMINFERENCESERVICE,
            llm_isvc.metadata.name,
        )
    except client.rest.ApiException as e:
        raise RuntimeError(
            f"Exception when calling CustomObjectsApi->"
            f"delete_namespaced_custom_object for LLMInferenceService: {e}"
        ) from e


def get_llmisvc(kserve_client: KServeClient, name, namespace, version=constants.KSERVE_V1ALPHA1_VERSION):
    try:
        return kserve_client.api_instance.get_namespaced_custom_object(
            constants.KSERVE_GROUP,
            version,
            namespace,
            KSERVE_PLURAL_LLMINFERENCESERVICE,
            name,
        )
    except client.rest.ApiException as e:
        raise RuntimeError(
            f"Exception when calling CustomObjectsApi->"
            f"get_namespaced_custom_object for LLMInferenceService: {e}"
        ) from e


def wait_for_model_response(
    kserve_client: KServeClient,
    test_case: Case,
    timeout_seconds: int = 300, # TODO Make it configurable in Case
) -> str:
    
    service_url = None

    def assert_model_responds():
        nonlocal service_url

        try:
            service_url = get_llm_service_url(kserve_client, test_case.llm_service)
        except Exception as e:
            raise AssertionError(f"Failed to get service URL: {e}") from e

        completion_url = f"{service_url}/v1/completions"
        test_payload = {"model": test_case.model_name, "prompt": test_case.prompt, "max_tokens": test_case.max_tokens}
        print(f"Calling LLM service at {completion_url} with payload {test_payload}")
        try:
            response = requests.post(
                completion_url,
                headers={"Content-Type": "application/json"},
                json=test_payload,
                timeout=30, # TODO Make it configurable
            )
        except Exception as e:
            raise AssertionError(f"Failed to call model: {e}") from e

        test_case.response_assertion(response)
        return service_url

    return wait_for(assert_model_responds, timeout=timeout_seconds, interval=10.0)


def get_llm_service_url(kserve_client: KServeClient, llm_isvc: V1alpha1LLMInferenceService):
    service_name = llm_isvc.metadata.name
    
    try:
        llm_isvc = get_llmisvc(kserve_client, llm_isvc.metadata.name, llm_isvc.metadata.namespace, llm_isvc.api_version.split("/")[1])

        if "status" not in llm_isvc:
            raise ValueError(f"No status found in LLM inference service {service_name} status: {llm_isvc}")

        status = llm_isvc["status"]

        if "url" in status and status["url"]:
            return status["url"]

        if (
            "addresses" in status
            and status["addresses"]
            and len(status["addresses"]) > 0
        ):
            first_address = status["addresses"][0]
            if "url" in first_address:
                return first_address["url"]

        raise ValueError(f"No URL found in LLM inference service {service_name} status")

    except Exception as e:
        raise ValueError(
            f"Failed to get URL for LLM inference service {service_name}: {e}"
        ) from e


def wait_for(assertion_fn: Callable[[], Any], timeout: float = 5.0, interval: float = 0.1) -> Any:
    """Wait for assertion function to succeed within timeout."""
    deadline = time.time() + timeout
    while True:
        try:
            return assertion_fn()
        except AssertionError:
            if time.time() >= deadline:
                raise
            time.sleep(interval)

def collect_diagnostics(kserve_client: KServeClient, llm_isvc: V1alpha1LLMInferenceService):
    try:
        
        service_name = llm_isvc.metadata.name
        namespace = llm_isvc.metadata.namespace
        print(f"\n{'='*60}")
        print(f"DIAGNOSTIC INFORMATION FOR {service_name} in {namespace}")
        print(f"{'='*60}")

        print("\n--- LLM Inference Service ---")
        try:
            llm_isvc = get_llmisvc(kserve_client, service_name, namespace)
            print(json.dumps(llm_isvc, indent=2, default=str))
        except Exception as e:
            print(f"Failed to get LLM inference service: {e}")

        print("\n--- Events ---")
        try:
            core_v1 = client.CoreV1Api()
            events = core_v1.list_namespaced_event(
                namespace=namespace,
                field_selector=f"involvedObject.name={service_name}",
            )
            if events.items:
                sorted_events = sorted(
                    events.items,
                    key=lambda x: x.last_timestamp or x.first_timestamp,
                    reverse=True,
                )
                for event in sorted_events[:5]:
                    timestamp = event.last_timestamp or event.first_timestamp
                    print(f"  {event.type}: {event.reason} - {event.message}")
                    print(f"    Time: {timestamp}")
            else:
                print("  No events found")
        except Exception as e:
            print(f"Failed to list events: {e}")

        print(f"\n{'='*60}")

    except Exception as e:
        print(f"Failed to collect diagnostics: {e}")
