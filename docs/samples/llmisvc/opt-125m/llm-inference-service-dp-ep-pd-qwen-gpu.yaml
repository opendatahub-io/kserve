apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: qwen3-30b-a3b-fp8
  annotations:
    leaderworkerset.sigs.k8s.io/subgroup-exclusive-topology: kubernetes.io/hostname
spec:
  model:
    uri: hf://deepseek-ai/DeepSeek-R1-0528
    name: deepseek-ai/DeepSeek-R1-0528
  parallelism:
    data: 1
    dataLocal: 8
    expert: true
    tensor: 1
  router:
    scheduler: { }
    route: { }
    gateway: { }
  template:
    containers:
      - name: main
        env:
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          # Disabled because of https://github.com/vllm-project/vllm/pull/21517 and llm-d 0.2 doesn't include the fix.
          - name: VLLM_USE_DEEP_GEMM
            value: "0"
          - name: TRITON_LIBCUDA_PATH
            value: /usr/lib64
          - name: HF_HUB_DISABLE_XET
            value: "1"
          - name: VLLM_SKIP_P2P_CHECK
            value: "1"
          - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
            value: "1"
          - name: VLLM_USE_DEEP_GEMM
            value: "1"
          - name: VLLM_ALL2ALL_BACKEND
            value: deepep_high_throughput
          - name: NVIDIA_GDRCOPY
            value: enabled
          - name: NVSHMEM_DEBUG
            value: INFO
          - name: NVSHMEM_REMOTE_TRANSPORT
            value: ibgda
          - name: NVSHMEM_IB_ENABLE_IBGDA
            value: "true"
          - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
            value: eth0
          - name: GLOO_SOCKET_IFNAME
            value: eth0
          - name: NCCL_SOCKET_IFNAME
            value: eth0
          - name: NCCL_IB_HCA
            value: ibp
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: HF_HUB_CACHE
            value: /huggingface-cache
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        resources:
          limits:
            cpu: 16
            ephemeral-storage: 64Gi
            memory: 512Gi
            nvidia.com/gpu: "8"
            rdma/ib: 1
          requests:
            cpu: 8
            ephemeral-storage: 32Gi
            memory: 256Gi
            nvidia.com/gpu: "8"
            rdma/ib: 1
  worker:
    containers:
      - name: main
        env:
          - name: VLLM_LOGGING_LEVEL
            value: DEBUG
          # Disabled because of https://github.com/vllm-project/vllm/pull/21517 and llm-d 0.2 doesn't include the fix.
          - name: VLLM_USE_DEEP_GEMM
            value: "0"
          - name: TRITON_LIBCUDA_PATH
            value: /usr/lib64
          - name: HF_HUB_DISABLE_XET
            value: "1"
          - name: VLLM_SKIP_P2P_CHECK
            value: "1"
          - name: VLLM_RANDOMIZE_DP_DUMMY_INPUTS
            value: "1"
          - name: VLLM_USE_DEEP_GEMM
            value: "1"
          - name: VLLM_ALL2ALL_BACKEND
            value: deepep_high_throughput
          - name: NVIDIA_GDRCOPY
            value: enabled
          - name: NVSHMEM_DEBUG
            value: INFO
          - name: NVSHMEM_REMOTE_TRANSPORT
            value: ibgda
          - name: NVSHMEM_IB_ENABLE_IBGDA
            value: "true"
          - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
            value: eth0
          - name: GLOO_SOCKET_IFNAME
            value: eth0
          - name: NCCL_SOCKET_IFNAME
            value: eth0
          - name: NCCL_IB_HCA
            value: ibp
          - name: VLLM_LOGGING_LEVEL
            value: INFO
          - name: HF_HUB_CACHE
            value: /huggingface-cache
          - name: VLLM_NIXL_SIDE_CHANNEL_HOST
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
        resources:
          limits:
            cpu: 16
            ephemeral-storage: 64Gi
            memory: 512Gi
            nvidia.com/gpu: "8"
            rdma/ib: 1
          requests:
            cpu: 8
            ephemeral-storage: 32Gi
            memory: 256Gi
            nvidia.com/gpu: "8"
            rdma/ib: 1
